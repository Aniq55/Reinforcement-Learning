\documentclass[11pt,a4paper]{article}

\usepackage[applemac]{inputenc}
\usepackage{latexsym}
\usepackage{graphics}
\usepackage[francais]{babel}
\usepackage{amsmath,amssymb}
\usepackage{pstricks,pst-plot}
\usepackage{calc}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[T1]{fontenc}
\usepackage{stmaryrd}
\pagestyle{plain}



\title{Programming Assignment 1 \\ From planning to reinforcement learning}
\author{Mathurin \textsc{Massias} \and Clément \textsc{Nicolle}}
\date{\today} 

\begin{document}
\maketitle

\section{Simulated trajectories and estimation of value function}
\hspace{-6mm}\textbf{Question 1}: At a given week, the shop owner has what he had last week, plus what he received (and this sum cannot be greater than the maximum he can stock), minus what he sold, and he cannot sell more than he has, so this difference cannot be negative.
\\The reward at a given week is equal to: minus the fix cost of delivery, minus the cost of his command, minus maintenance cost, plus the sales.

\section{Policy evaluation}
\hspace{-6mm}We wrote three functions:
\\- pol\_eval\_1(pi, P, R, gamma) computes V using the matricial equation $V^{\pi} = R + \gamma PV^{\pi}$, which leads to $V^{\pi} = (I - \gamma P)^{-1} R$. In these equations $R$ and $P$ are not the general 2D and 3D matrices, but the 1D and 2D matrices associated to a given policy $\pi$ = $R = R(x, \pi(x))$ and $P = P(x, y, \pi(x))$.
\\- pol\_eval\_2(MCn,n,V0,pi,D,M,K,h,c,pr,gamma) computes an estimation of $V$ using MCn Monte-Carlo simulation for each coordinate. It is slow, converges slowly (in $\sqrt{n}$ by LLN, and does not provide an exact value of $V^{\pi}$.
\\- pol\_eval\_3(pi, P, R ,gamma, n\_it) compute $V^{\pi}$ using n\_it iterations of the Bellman operator. Same here, $P$ and $R$ are adapted to the peculiar policy $\pi$.


\section{Value iteration, policy iteration}


\end{document}